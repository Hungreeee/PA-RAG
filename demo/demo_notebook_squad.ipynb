{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Short-form QnA\n",
    "\n",
    "In this notebook, we aim to construct and evaluate the performance of a Q-RAG and RAG agent. While in a realistic use case, it would have require developers to set up graph-like databases to build the core elements of Q-RAG (e.g., question-chunk connections), this notebook show that Q-RAG can be easily replicated using vectorstore and document metadata. Here, we would like to utilize LangChain - a library providing wrappers for framework that are very easy and quick to set up, which is useful for demo purposes.\n",
    "In this notebook, we aim to construct and evaluate the performance of a Q-RAG and RAG agent. While in a realistic use case, it would have require developers to set up graph-like databases to build the core elements of Q-RAG (e.g., question-chunk connections), this notebook show that Q-RAG can be easily replicated using vectorstore and document metadata. Here, we would like to utilize LangChain - a library providing wrappers for framework that are very easy and quick to set up, which is useful for demo purposes.\n",
    "\n",
    "### Set-up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from copy import deepcopy\n",
    "import json\n",
    "import re\n",
    "import random\n",
    "\n",
    "from typing import List\n",
    "\n",
    "from datasets import load_dataset, Dataset\n",
    "\n",
    "from nltk import word_tokenize\n",
    "from nltk.translate.bleu_score import sentence_bleu, SmoothingFunction\n",
    "\n",
    "from langchain_community.document_loaders import DataFrameLoader\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.documents.base import Document\n",
    "from langchain_openai import ChatOpenAI"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some important technical details before we start:\n",
    "\n",
    "- LLM: `gpt-4o-mini`\n",
    "- Vectorstore: `FAISS`\n",
    "- Embedding Model: `sentence-transformers/all-MiniLM-L6-v2`\n",
    "- Evaluation Dataset: `rajpurkar/squad` (HuggingFace)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "OPENAI_API_KEY = \"\"\n",
    "OPENAI_MODEL = \"gpt-4o-mini\"\n",
    "\n",
    "EMBEDDING_MODEL = \"sentence-transformers/all-MiniLM-L6-v2\"\n",
    "\n",
    "DATASET = \"rajpurkar/squad\"\n",
    "\n",
    "FAISS_PATH = \"./vectorstore/squad\"\n",
    "RESULTS_PATH = \"./results/squad\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Embedding Model\n",
    "\n",
    "Here we want to load our embedding model `all-MiniLM-L6-v2`, which is a good option due to its light-weight, enabling fast inference. To enable access to everyone, the default device to be loaded is the CPU. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_model = HuggingFaceEmbeddings(\n",
    "    model_name=EMBEDDING_MODEL, \n",
    "    model_kwargs={\"device\": \"cpu\"}\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, we take the validation split of the dataset for evaluation. Additionally, we only sample around 200 datapoints out of the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['id', 'title', 'context', 'question', 'answers'],\n",
       "    num_rows: 1000\n",
       "})"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "full_dataset = load_dataset(DATASET, split=\"validation\")\n",
    "\n",
    "random.seed(13)\n",
    "rand_indices = random.sample(range(0, len(full_dataset)), 1000)\n",
    "dataset = full_dataset.select(rand_indices)\n",
    "\n",
    "dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we create a corpus, grouping the existing `context` chunks provided in the original dataset together."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Index: 2067 entries, 0 to 10565\n",
      "Data columns (total 4 columns):\n",
      " #   Column  Non-Null Count  Dtype \n",
      "---  ------  --------------  ----- \n",
      " 0   id      2067 non-null   object\n",
      " 1   title   2067 non-null   object\n",
      " 2   text    2067 non-null   object\n",
      " 3   type    2067 non-null   object\n",
      "dtypes: object(4)\n",
      "memory usage: 80.7+ KB\n"
     ]
    }
   ],
   "source": [
    "document_df = pd.DataFrame({\n",
    "    \"id\": full_dataset[\"id\"],\n",
    "    \"title\": full_dataset[\"title\"],\n",
    "    \"text\": full_dataset[\"context\"],\n",
    "    \"type\": \"chunk\"\n",
    "})\n",
    "\n",
    "document_df = document_df.drop_duplicates([\"text\"])\n",
    "document_df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The documents are then further splited into small chunks (size of around 300 chars) for better retrieval. This chunk corpus will then be loaded into a FAISS instance, which are stored locally. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "loader = DataFrameLoader(document_df, page_content_column=\"text\")\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size = 300,\n",
    "    chunk_overlap = 100\n",
    ")\n",
    "\n",
    "document_chunks = loader.load()\n",
    "document_chunks = text_splitter.split_documents(document_chunks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorstore_db = FAISS.from_documents(\n",
    "    documents=document_chunks, \n",
    "    embedding=embedding_model, \n",
    "    normalize_L2=True\n",
    ")\n",
    "\n",
    "vectorstore_db.save_local(f\"{FAISS_PATH}/chunk-vectorstore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Retriever"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we would like to initiate our vectorstore retrievers. For this design, let us have two main retrievers serving different purposes:\n",
    "\n",
    "- Chunk vectorstore: This will be used to obtain relevant chunks given a query as normally. We only need to load the indexed vectors from our local storage.\n",
    "- Question vectorstores: This will be used to retrieve similar \"model questions\". We will need to create a new instance of FAISS vectorstore, with a default \"mock question\" inside (simply because it is not possible to create an empty database)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "chunks_vectorstore_db = FAISS.load_local(\n",
    "    folder_path=f\"{FAISS_PATH}/chunk-vectorstore\", \n",
    "    embeddings=embedding_model, \n",
    "    normalize_L2=True,\n",
    "    allow_dangerous_deserialization=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We should pay attention to the structure of the \"Mock Question\" template. Its metadata has a list-type attribute called `connections`, which later can be used to store IDs of relevant documents. This helps to replicate the model question-chunk connections in Q-RAG. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "question_vectorstore_db = FAISS.from_documents(\n",
    "    documents=[\n",
    "        Document(page_content=\"Mock Question\", metadata={\n",
    "            \"id\": 0, \n",
    "            \"type\": \"question\", \n",
    "            \"connections\": []\n",
    "        })],\n",
    "    embedding=embedding_model\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LLM Client"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This cell loads the OpenAI's `gpt-3.5-turbo` as our default LLM client."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm_agent = ChatOpenAI(\n",
    "    api_key=OPENAI_API_KEY,\n",
    "    model=OPENAI_MODEL,\n",
    "    temperature=0.0\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RAG Agent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A RAG Agent class template is created here, unifying the retrievers and the generator into one pipeline. This agent has the following functionality:\n",
    "\n",
    "- Utilize the chunk database / question database to perform retrieval. If one recalls, besides the generic chunk retrieval process, Q-RAG also attempts to retrieve relevant questions to obtain the connected context chunks. \n",
    "- Construct the context string from the raw retrieval results to augment the input user query. The complete prompt will then be processed by LLM generator to obtain answers.\n",
    "\n",
    "As a side note, the SQuAD dataset focuses on short-form question-answering, meaning that the answers will be very short and concise. Here, we engineer our prompt template to have the same behavior. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RAGAgent:\n",
    "    def __init__(\n",
    "        self,\n",
    "        client: ChatOpenAI,\n",
    "        chunk_retriever: FAISS,\n",
    "        question_retriever: FAISS,\n",
    "        dataframe: pd.DataFrame\n",
    "    ):\n",
    "        self.dataframe = dataframe\n",
    "        self.client = client\n",
    "        self.chunk_retriever = chunk_retriever\n",
    "        self.question_retriever = question_retriever\n",
    "\n",
    "    \n",
    "    def retrieve_context(\n",
    "        self,\n",
    "        query: str,\n",
    "        top_k: int,\n",
    "        distance_threshold: float,\n",
    "        retrieve_questions: bool = False,\n",
    "        **krawgs\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Retrieve relevant chunks/questions given query\n",
    "        \"\"\"\n",
    "        retriever = self.chunk_retriever if not retrieve_questions else self.question_retriever\n",
    "        \n",
    "        docs_with_metadata = retriever.similarity_search_with_score(\n",
    "            query=query,\n",
    "            k=top_k,\n",
    "            **krawgs\n",
    "        )\n",
    "        filtered_docs = [doc for doc, score in docs_with_metadata if score <= distance_threshold] \n",
    "        \n",
    "        if retrieve_questions:\n",
    "            doc_ids = [doc for relevant_docs in filtered_docs for doc in relevant_docs.metadata[\"connections\"]]\n",
    "        else:\n",
    "            doc_ids = [doc.metadata[\"id\"] for doc in filtered_docs]\n",
    "\n",
    "        filtered_docs = [DataFrameLoader(pd.DataFrame(self.dataframe[self.dataframe[\"id\"] == doc_id])).load()[0] for doc_id in set(doc_ids)]\n",
    "        \n",
    "        return filtered_docs\n",
    "    \n",
    "\n",
    "    def generate_response(\n",
    "        self,\n",
    "        question: str,\n",
    "        retrieved_docs: List[Document]\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Generate response based on input query and raw context documents\n",
    "        \"\"\"\n",
    "        documents = [\"Title:\" + str(doc.metadata[\"title\"]) + \"\\n\" + str(doc.page_content) for doc in retrieved_docs]\n",
    "        context_str = \"\\n\\n\".join(documents)\n",
    "    \n",
    "        prompt = ChatPromptTemplate.from_messages([\n",
    "             (\"system\", \"\"\"\n",
    "                Find the answer to the user's QUESTION by using relevant keywords or phrases from the CONTEXT above. \n",
    "                Keep the answer extremely short. \n",
    "                You are not allowed to include any of your own words. You are only allowed to use words from the CONTEXT. \n",
    "              \"\"\"),\n",
    "            (\"human\", \"### CONTEXT\\n{context}\\n### QUESTION\\n{question}\")\n",
    "        ])\n",
    "\n",
    "        chain = prompt | self.client\n",
    "        response = chain.invoke({\"question\": question, \"context\": context_str})\n",
    "\n",
    "        return response.content\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Helper functions\n",
    "\n",
    "Before we start"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Two evaluation metrics are utilized:\n",
    "\n",
    "- BLEU score: measure the n-gram precision between a candidate text with a list of references. This should give us a rough idea of the quality of our generated answers compared to the ground truth answers.\n",
    "- Retrieval score: it's just a made-up score that checks whether the ground truth context has been retrieved or not."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_bleu_score(ref_list: str, cand: str):\n",
    "    \"\"\"\n",
    "    BLEU score\n",
    "    \"\"\"\n",
    "    smoothing_func = SmoothingFunction().method1\n",
    "\n",
    "    reference = [word_tokenize(re.sub(r'[^\\w\\s]','', ref.lower())) for ref in ref_list]\n",
    "    candidate = word_tokenize(re.sub(r'[^\\w\\s]','', cand.lower()))\n",
    "\n",
    "    weight_configs = (1, 0, 0, 0)\n",
    "\n",
    "    bleu_score = sentence_bleu(\n",
    "        references=reference, \n",
    "        hypothesis=candidate, \n",
    "        weights=weight_configs, \n",
    "        smoothing_function=smoothing_func\n",
    "    )\n",
    "\n",
    "    return bleu_score\n",
    "\n",
    "\n",
    "def compute_retrieval_score(ground_truth_context: str, retrieved_context: List[Document]):\n",
    "    \"\"\"\n",
    "    Whether the ground truth context document is retrieved \n",
    "    \"\"\"\n",
    "    retrieved_context_str = [doc.page_content for doc in retrieved_context]\n",
    "    return 1 if ground_truth_context in retrieved_context_str else 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This cell creates an evaluation loop, which iterates through each row in the dataset to generate the results and evaluate them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluation(\n",
    "    agent: RAGAgent, \n",
    "    test_set: Dataset, \n",
    "    top_k: int, \n",
    "    q_top_k: int,\n",
    "    distance_threshold: float,\n",
    "    q_distance_threshold: float\n",
    "):\n",
    "    results = []\n",
    "\n",
    "    for i in range(len(test_set)):\n",
    "        id = test_set[i][\"id\"]\n",
    "        question = test_set[i][\"question\"]\n",
    "        ground_truth_answer = test_set[i][\"answers\"][\"text\"]\n",
    "        ground_truth_context = test_set[i][\"context\"]\n",
    "\n",
    "        relevant_chunks = agent.retrieve_context(\n",
    "            query=question, \n",
    "            top_k=top_k, \n",
    "            distance_threshold=distance_threshold,\n",
    "            retrieve_questions=False\n",
    "        )\n",
    "\n",
    "        relevant_question_chunks = agent.retrieve_context(\n",
    "            query=question, \n",
    "            top_k=q_top_k, \n",
    "            distance_threshold=q_distance_threshold,\n",
    "            retrieve_questions=True\n",
    "        )\n",
    "\n",
    "        context = relevant_question_chunks + relevant_chunks\n",
    "        response = agent.generate_response(question, context)\n",
    "\n",
    "        bleu_score = compute_bleu_score(ground_truth_answer, response)\n",
    "        retrieval_score = compute_retrieval_score(ground_truth_context, context)\n",
    "\n",
    "        results.append({\n",
    "            \"question\": question,\n",
    "            \"answer\": response,\n",
    "            \"ground_truth_answer\": ground_truth_answer,\n",
    "            \"context\": [chunk.page_content for chunk in relevant_chunks],\n",
    "            \"q_context\": [chunk.page_content for chunk in relevant_question_chunks],\n",
    "            \"ground_truth_context\": ground_truth_context,\n",
    "            \"bleu_score\": bleu_score,\n",
    "            \"retrieval_score\": retrieval_score,\n",
    "            \"id\": id\n",
    "        })\n",
    "        \n",
    "    return results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Demo #1 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this demo, we evaluate the agent before and after the *corrective loop*.\n",
    "\n",
    "#### Evaluation Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['id', 'title', 'context', 'question', 'answers'],\n",
       "    num_rows: 1000\n",
       "})"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Naive RAG Agent Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We first initiate our baseline RAG Agent with access to the chunk database and the (empty) question database."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "rag_agent = RAGAgent(\n",
    "    client=llm_agent,\n",
    "    chunk_retriever=chunks_vectorstore_db,\n",
    "    question_retriever=question_vectorstore_db,\n",
    "    dataframe=document_df\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = evaluation(\n",
    "    agent=rag_agent,\n",
    "    test_set=dataset,\n",
    "    top_k=5,\n",
    "    q_top_k=1,\n",
    "    distance_threshold=1.5,\n",
    "    q_distance_threshold=1.5\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(f\"{RESULTS_PATH}/run-0-demo-1.json\", \"w\") as file:\n",
    "    file.write(json.dumps(results))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7147777497184254"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bleu_scores = [result[\"bleu_score\"] for result in results] \n",
    "np.mean(bleu_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.902"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "retrieval_scores = [result[\"retrieval_score\"] for result in results] \n",
    "np.mean(retrieval_scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Q-RAG Corrective Loop"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section, we are going to perform the corrective loop. This requires an external source to provide the ground truth answers. In reality, we can have human evaluators going through poor responses (e.g., responses reported by users, responses with low metric values) to correct them.\n",
    "\n",
    "We start by first create a copy of the initial question database (currently empty). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_question_vectorstore_db = deepcopy(question_vectorstore_db)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We then iterate through the results to detect poor answers, which are defined as BLEU < 0.5. For each of these instances, we are going to utilize the ground truth answer to retrieve the most relevant chunks. These chunks will then be put into the `connections` attribute of the corresponding questions. These questions documents are then appended to the question database. \n",
    "\n",
    "In the code, one may be able to observe that we are utilizing both the original question and the ground truth answer to retrieve relevant chunks here (Line 9). This is simply because SQuAD is a short-form QnA dataset, meaning that the expected answers are extremely short and concise, composing of just one or two keywords. Therefore, the answer alone is inappropirate to be used for context retrieval. As such, it is a reasonable option to concatenate the original question and the ground truth answer together for this task. In most cases, inlcluding just a few keywords from the corrected answers is enough to significantly improve the retrieval results. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, score in enumerate(bleu_scores):\n",
    "    if score < 0.6:\n",
    "        id = dataset[i][\"id\"]\n",
    "        question = dataset[i][\"question\"]\n",
    "        ground_truth_answer_list = sorted(dataset[i][\"answers\"][\"text\"], key=len)\n",
    "        ground_truth_answer = ground_truth_answer_list[-1]\n",
    "\n",
    "        relevant_chunk_list = rag_agent.retrieve_context(\n",
    "            query=question + \" \" + ground_truth_answer,\n",
    "            top_k=2,\n",
    "            distance_threshold=1.5\n",
    "        )\n",
    "\n",
    "        relevant_chunk_id_list = [chunk.metadata[\"id\"] for chunk in relevant_chunk_list]\n",
    "\n",
    "        question_document = Document(\n",
    "            page_content=question,\n",
    "            metadata={\n",
    "                \"id\": id,\n",
    "                \"type\": \"question\", \n",
    "                \"connections\": relevant_chunk_id_list\n",
    "            }\n",
    "        )\n",
    "\n",
    "        new_question_vectorstore_db.add_documents([question_document])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_question_vectorstore_db.save_local(f\"{FAISS_PATH}/question-vectorstore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_question_vectorstore_db = FAISS.load_local(\n",
    "    folder_path=f\"{FAISS_PATH}/question-vectorstore\", \n",
    "    embeddings=embedding_model, \n",
    "    normalize_L2=True,\n",
    "    allow_dangerous_deserialization=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Q-RAG Agent Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We create a new rag agent with access to the newly indexed question database."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_rag_agent = RAGAgent(\n",
    "    client=llm_agent,\n",
    "    chunk_retriever=chunks_vectorstore_db,\n",
    "    question_retriever=new_question_vectorstore_db,\n",
    "    dataframe=document_df\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A new evaluation process takes place for this RAG agent. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_results = evaluation(\n",
    "    agent=new_rag_agent,\n",
    "    test_set=dataset,\n",
    "    top_k=5,\n",
    "    q_top_k=2,\n",
    "    distance_threshold=1.5,\n",
    "    q_distance_threshold=1.5\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(f\"{RESULTS_PATH}/run-1-demo-1.json\", \"w\") as file:\n",
    "    file.write(json.dumps(new_results))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7225012975337309"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_bleu_scores = [result[\"bleu_score\"] for result in new_results] \n",
    "np.mean(new_bleu_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.958"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_retrieval_scores = [result[\"retrieval_score\"] for result in new_results] \n",
    "np.mean(new_retrieval_scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As one can observed, there has been some decent improvement in the average BLEU score and a significant improvement in context retrieval score. This implies that Q-RAG addressed the poor context retrieval cases quite well. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Demo #2\n",
    "\n",
    "The previous demo utilized the same question ingested in the database to evaluate Q-RAG, which may not be fair since the question can be directly matched with itself in the database. Therefore, in this demo, we will develop a new question dataset that are the rephrased version of the questions in the original dataset. This allows us to observe how Q-RAG performs when variants of the complex questions are asked. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### New Evaluation Dataset\n",
    "\n",
    "Here, we would like to select only the \"complex questions\" - ones with BLEU < 0.6 when evaluated using vanilla RAG. These are the same questions that has been ingested in the question vector database ealier. \n",
    "\n",
    "This approach would allow us to observe the retrieval performance of the chunk correction mechanism of Q-RAG."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['id', 'title', 'context', 'question', 'answers'],\n",
       "    num_rows: 344\n",
       "})"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "complex_question_dataset = Dataset.from_list([row for index, row in enumerate(dataset) if bleu_scores[index] < 0.6])\n",
    "complex_question_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This cell rephrases the questions in the dataset. This will help us to evaluate the performance of Q-RAG's question-based retrieval more realistically."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rephrase_question(row: str):\n",
    "    prompt = ChatPromptTemplate.from_messages([\n",
    "        (\"system\", \"\"\"\n",
    "            You are provided with a QUESTION. \n",
    "            Your task is to transform the QUESTION into a REPHRASED QUESTION by changing only the wording. \n",
    "            Make sure to leave enough information in the REPHRASED QUESTION so that the the answer to the do not change.\n",
    "        \"\"\"),\n",
    "        (\"user\", \"QUESTION: {question}\\nREPHRASED QUESTION:\")\n",
    "    ])\n",
    "\n",
    "    chain = prompt | llm_agent\n",
    "\n",
    "    rephrased_question = chain.invoke({\"question\": row[\"question\"]}).content\n",
    "    row[\"question\"] = re.sub(\"rephrased question:\", \"\", rephrased_question.lower()).strip()\n",
    "    \n",
    "    return row"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Parameter 'function'=<function rephrase_question at 0x000001F3D3712D40> of the transform datasets.arrow_dataset.Dataset._map_single couldn't be hashed properly, a random hash was used instead. Make sure your transforms and parameters are serializable with pickle or dill for the dataset fingerprinting and caching to work. If you reuse this transform, the caching mechanism will consider it to be different from the previous calls and recompute everything. This warning is only showed once. Subsequent hashing failures won't be showed.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2c1d14d1ba874ebd811eb4b1e8f96e4e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/344 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "rephrased_dataset = complex_question_dataset.map(rephrase_question)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Evaluation\n",
    "\n",
    "We evaluate 3 different scenarios:\n",
    "\n",
    "- Vanilla RAG on complex question dataset: This serves as a baseline for comparison.\n",
    "- Q-RAG on complex question dataset: This shows us how ingestion of the complex questions can improve the retrieval.\n",
    "- Q-RAG on rephrased complex question dataset: This shows us the performance of Q-RAG when met with variants of the complex questions. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Vanilla RAG on complex questions (Baseline)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "rag_results_complex = evaluation(\n",
    "    agent=rag_agent,\n",
    "    test_set=complex_question_dataset,\n",
    "    top_k=5,\n",
    "    q_top_k=2,\n",
    "    distance_threshold=1.5,\n",
    "    q_distance_threshold=1.5\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(f\"{RESULTS_PATH}/run-0-demo-2.json\", \"w\") as file:\n",
    "    file.write(json.dumps(rag_results_complex))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.2501785283062391"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean([result[\"bleu_score\"] for result in rag_results_complex])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7383720930232558"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean([result[\"retrieval_score\"] for result in rag_results_complex])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Q-RAG on complex questions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "qrag_results_complex = evaluation(\n",
    "    agent=new_rag_agent,\n",
    "    test_set=complex_question_dataset,\n",
    "    top_k=5,\n",
    "    q_top_k=2,\n",
    "    distance_threshold=1.5,\n",
    "    q_distance_threshold=1.5\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(f\"{RESULTS_PATH}/run-1-demo-2.json\", \"w\") as file:\n",
    "    file.write(json.dumps(qrag_results_complex))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.39212412498484117"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean([result[\"bleu_score\"] for result in qrag_results_complex])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.902"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean([result[\"retrieval_score\"] for result in results])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Q-RAG on rephrased complex questions "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "qrag_results_rephrased = evaluation(\n",
    "    agent=new_rag_agent,\n",
    "    test_set=rephrased_dataset,\n",
    "    top_k=5,\n",
    "    q_top_k=2,\n",
    "    distance_threshold=1.5,\n",
    "    q_distance_threshold=1.5\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(f\"{RESULTS_PATH}/run-2-demo-2.json\", \"w\") as file:\n",
    "    file.write(json.dumps(qrag_results_rephrased))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.39325279700091936"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean([result[\"bleu_score\"] for result in qrag_results_rephrased])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8924418604651163"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean([result[\"retrieval_score\"] for result in qrag_results_rephrased])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Depending on how the example question are rephrased, the results may vary. Nevertheless, it can be observed that the Q-RAG agent was able to obtain a highly similar performance (to the original complex question dataset) when performing on the rephrased question dataset. This implies that the question-based retrieval mechanism of Q-RAG is very stable in practical."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
