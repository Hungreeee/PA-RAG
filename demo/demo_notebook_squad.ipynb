{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Short-form QnA\n",
    "\n",
    "### Set-up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from copy import deepcopy\n",
    "import json\n",
    "import re\n",
    "import random\n",
    "\n",
    "from typing import List\n",
    "\n",
    "from datasets import load_dataset, Dataset\n",
    "\n",
    "from nltk import word_tokenize\n",
    "from nltk.translate.bleu_score import sentence_bleu, SmoothingFunction\n",
    "\n",
    "from langchain_community.document_loaders import DataFrameLoader\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from langchain_community.vectorstores.faiss import DistanceStrategy\n",
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.documents.base import Document\n",
    "from langchain_openai import ChatOpenAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "OPENAI_API_KEY = \"\"\n",
    "OPENAI_MODEL = \"gpt-3.5-turbo\"\n",
    "\n",
    "EMBEDDING_MODEL = \"sentence-transformers/all-MiniLM-L6-v2\"\n",
    "\n",
    "DATASET = \"rajpurkar/squad\"\n",
    "\n",
    "FAISS_PATH = \"./vectorstore/squad\"\n",
    "RESULTS_PATH = \"./results/squad\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Embedding Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_model = HuggingFaceEmbeddings(\n",
    "    model_name=EMBEDDING_MODEL, \n",
    "    model_kwargs={\"device\": \"cpu\"}\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['id', 'title', 'context', 'question', 'answers'],\n",
       "    num_rows: 200\n",
       "})"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "full_dataset = load_dataset(DATASET, split=\"validation\")\n",
    "\n",
    "random.seed(13)\n",
    "rand_indices = random.sample(range(0, len(full_dataset)), 200)\n",
    "dataset = full_dataset.select(rand_indices)\n",
    "\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Index: 2067 entries, 0 to 10565\n",
      "Data columns (total 4 columns):\n",
      " #   Column  Non-Null Count  Dtype \n",
      "---  ------  --------------  ----- \n",
      " 0   id      2067 non-null   object\n",
      " 1   title   2067 non-null   object\n",
      " 2   text    2067 non-null   object\n",
      " 3   type    2067 non-null   object\n",
      "dtypes: object(4)\n",
      "memory usage: 80.7+ KB\n"
     ]
    }
   ],
   "source": [
    "document_df = pd.DataFrame({\n",
    "    \"id\": full_dataset[\"id\"],\n",
    "    \"title\": full_dataset[\"title\"],\n",
    "    \"text\": full_dataset[\"context\"],\n",
    "    \"type\": \"chunk\"\n",
    "})\n",
    "\n",
    "document_df = document_df.drop_duplicates([\"text\"])\n",
    "document_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "loader = DataFrameLoader(document_df, page_content_column=\"text\")\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size = 300,\n",
    "    chunk_overlap = 100\n",
    ")\n",
    "\n",
    "document_chunks = loader.load()\n",
    "document_chunks = text_splitter.split_documents(document_chunks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorstore_db = FAISS.from_documents(\n",
    "    documents=document_chunks, \n",
    "    embedding=embedding_model, \n",
    "    normalize_L2=True\n",
    ")\n",
    "\n",
    "vectorstore_db.save_local(FAISS_PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Retriever"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "chunks_vectorstore_db = FAISS.load_local(\n",
    "    folder_path=FAISS_PATH, \n",
    "    embeddings=embedding_model, \n",
    "    normalize_L2=True,\n",
    "    allow_dangerous_deserialization=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "question_vectorstore_db = FAISS.from_documents(\n",
    "    documents=[\n",
    "        Document(page_content=\"Mock Question\", metadata={\n",
    "            \"id\": 0, \n",
    "            \"type\": \"question\", \n",
    "            \"connections\": []\n",
    "        })],\n",
    "    embedding=embedding_model\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LLM Client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm_agent = ChatOpenAI(\n",
    "    api_key=OPENAI_API_KEY,\n",
    "    model=OPENAI_MODEL,\n",
    "    temperature=0.0\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RAG Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RAGAgent:\n",
    "    def __init__(\n",
    "        self,\n",
    "        client: ChatOpenAI,\n",
    "        chunk_retriever: FAISS,\n",
    "        question_retriever: FAISS,\n",
    "        dataframe: pd.DataFrame\n",
    "    ):\n",
    "        self.dataframe = dataframe\n",
    "        self.client = client\n",
    "        self.chunk_retriever = chunk_retriever\n",
    "        self.question_retriever = question_retriever\n",
    "\n",
    "    \n",
    "    def retrieve_context(\n",
    "        self,\n",
    "        query: str,\n",
    "        top_k: int,\n",
    "        distance_threshold: float,\n",
    "        retrieve_questions: bool = False,\n",
    "        **krawgs\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Retrieve relevant documents given query\n",
    "        \"\"\"\n",
    "        retriever = self.chunk_retriever if not retrieve_questions else self.question_retriever\n",
    "        \n",
    "        docs_with_metadata = retriever.similarity_search_with_score(\n",
    "            query=query,\n",
    "            k=top_k,\n",
    "            **krawgs\n",
    "        )\n",
    "        filtered_docs = [doc for doc, score in docs_with_metadata if score <= distance_threshold] \n",
    "        \n",
    "        if retrieve_questions:\n",
    "            doc_ids = [doc for relevant_docs in filtered_docs for doc in relevant_docs.metadata[\"connections\"]]\n",
    "        else:\n",
    "            doc_ids = [doc.metadata[\"id\"] for doc in filtered_docs]\n",
    "\n",
    "        filtered_docs = [DataFrameLoader(pd.DataFrame(self.dataframe[self.dataframe[\"id\"] == doc_id])).load()[0] for doc_id in set(doc_ids)]\n",
    "        \n",
    "        return filtered_docs\n",
    "    \n",
    "\n",
    "    def generate_response(\n",
    "        self,\n",
    "        question: str,\n",
    "        retrieved_docs: List[Document]\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Generate response based on query and context documents\n",
    "        \"\"\"\n",
    "        documents = [\"Title:\" + str(doc.metadata[\"title\"]) + \"\\n\" + str(doc.page_content) for doc in retrieved_docs]\n",
    "        context_str = \"\\n\\n\".join(documents)\n",
    "    \n",
    "        prompt = ChatPromptTemplate.from_messages([\n",
    "             (\"system\", \"\"\"\n",
    "                ### INSTRUCTION\n",
    "                Answer the users QUESTION by extracting from the CONTEXT text above. \n",
    "                You should only use keywords from the provided CONTEXT to form your answer.\n",
    "                Keep your answer concise and short, just in a few words.\n",
    "              \"\"\"),\n",
    "            (\"human\", \"### CONTEXT\\n{context}\\n### QUESTION\\n{question}\")\n",
    "        ])\n",
    "\n",
    "        chain = prompt | self.client\n",
    "        response = chain.invoke({\"question\": question, \"context\": context_str})\n",
    "\n",
    "        return response.content\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "rag_agent = RAGAgent(\n",
    "    client=llm_agent,\n",
    "    chunk_retriever=chunks_vectorstore_db,\n",
    "    question_retriever=question_vectorstore_db,\n",
    "    dataframe=document_df\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_bleu_score(ref_list: str, cand: str):\n",
    "    \"\"\"\n",
    "    BLEU score\n",
    "    \"\"\"\n",
    "    smoothing_func = SmoothingFunction().method1\n",
    "\n",
    "    reference = [word_tokenize(re.sub(r'[^\\w\\s]','', ref.lower())) for ref in ref_list]\n",
    "    candidate = word_tokenize(re.sub(r'[^\\w\\s]','', cand.lower()))\n",
    "\n",
    "    weight_configs = (1, 0, 0, 0)\n",
    "\n",
    "    bleu_score = sentence_bleu(\n",
    "        references=reference, \n",
    "        hypothesis=candidate, \n",
    "        weights=weight_configs, \n",
    "        smoothing_function=smoothing_func\n",
    "    )\n",
    "\n",
    "    return bleu_score\n",
    "\n",
    "\n",
    "def compute_retrieval_score(ground_truth_context: str, retrieved_context: List[Document]):\n",
    "    \"\"\"\n",
    "    Whether the ground truth context document is retrieved \n",
    "    \"\"\"\n",
    "    retrieved_context_str = [doc.page_content for doc in retrieved_context]\n",
    "    return 1 if ground_truth_context in retrieved_context_str else 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluation(\n",
    "    agent: RAGAgent, \n",
    "    test_set: Dataset, \n",
    "    top_k: int, \n",
    "    q_top_k: int,\n",
    "    distance_threshold: float,\n",
    "    q_distance_threshold: float\n",
    "):\n",
    "    results = []\n",
    "\n",
    "    for i in range(len(test_set)):\n",
    "        id = test_set[i][\"id\"]\n",
    "        question = test_set[i][\"question\"]\n",
    "        ground_truth_answer = test_set[i][\"answers\"][\"text\"]\n",
    "        ground_truth_context = test_set[i][\"context\"]\n",
    "\n",
    "        relevant_chunks = agent.retrieve_context(\n",
    "            query=question, \n",
    "            top_k=top_k, \n",
    "            distance_threshold=distance_threshold,\n",
    "            retrieve_questions=False\n",
    "        )\n",
    "\n",
    "        relevant_question_chunks = agent.retrieve_context(\n",
    "            query=question, \n",
    "            top_k=q_top_k, \n",
    "            distance_threshold=q_distance_threshold,\n",
    "            retrieve_questions=True\n",
    "        )\n",
    "\n",
    "        context = relevant_question_chunks + relevant_chunks\n",
    "        response = agent.generate_response(question, context)\n",
    "\n",
    "        bleu_score = compute_bleu_score(ground_truth_answer, response)\n",
    "        retrieval_score = compute_retrieval_score(ground_truth_context, context)\n",
    "\n",
    "        results.append({\n",
    "            \"question\": question,\n",
    "            \"answer\": response,\n",
    "            \"ground_truth_answer\": ground_truth_answer,\n",
    "            \"context\": [chunk.page_content for chunk in relevant_chunks],\n",
    "            \"q_context\": [chunk.page_content for chunk in relevant_question_chunks],\n",
    "            \"ground_truth_context\": ground_truth_context,\n",
    "            \"bleu_score\": bleu_score,\n",
    "            \"retrieval_score\": retrieval_score,\n",
    "            \"id\": id\n",
    "        })\n",
    "        \n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = evaluation(\n",
    "    agent=rag_agent,\n",
    "    test_set=dataset,\n",
    "    top_k=5,\n",
    "    q_top_k=1,\n",
    "    distance_threshold=1.5,\n",
    "    q_distance_threshold=0.25\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(f\"{RESULTS_PATH}/run-0.json\", \"w\") as file:\n",
    "    file.write(json.dumps(results))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7302206150734558"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bleu_scores = [result[\"bleu_score\"] for result in results] \n",
    "np.mean(bleu_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.925"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "retrieval_scores = [result[\"retrieval_score\"] for result in results] \n",
    "np.mean(retrieval_scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q-RAG Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_question_vectorstore_db = deepcopy(question_vectorstore_db)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, score in enumerate(bleu_scores):\n",
    "    if score < 0.5:\n",
    "        id = dataset[i][\"id\"]\n",
    "        question = dataset[i][\"question\"]\n",
    "        ground_truth_answer_list = sorted(dataset[i][\"answers\"][\"text\"], key=len)\n",
    "        ground_truth_answer = ground_truth_answer_list[-1]\n",
    "\n",
    "        relevant_chunk_list = rag_agent.retrieve_context(\n",
    "            query=question + \" \" + ground_truth_answer,\n",
    "            top_k=2,\n",
    "            distance_threshold=1.5\n",
    "        )\n",
    "\n",
    "        relevant_chunk_id_list = [chunk.metadata[\"id\"] for chunk in relevant_chunk_list]\n",
    "\n",
    "        question_document = Document(\n",
    "            page_content=question,\n",
    "            metadata={\n",
    "                \"id\": id,\n",
    "                \"type\": \"question\", \n",
    "                \"connections\": relevant_chunk_id_list\n",
    "            }\n",
    "        )\n",
    "\n",
    "        new_question_vectorstore_db.add_documents([question_document])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_rag_agent = RAGAgent(\n",
    "    client=llm_agent,\n",
    "    chunk_retriever=chunks_vectorstore_db,\n",
    "    question_retriever=new_question_vectorstore_db,\n",
    "    dataframe=document_df\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_results = evaluation(\n",
    "    agent=new_rag_agent,\n",
    "    test_set=dataset,\n",
    "    top_k=5,\n",
    "    q_top_k=2,\n",
    "    distance_threshold=1.5,\n",
    "    q_distance_threshold=0.25\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(f\"{RESULTS_PATH}/run-1.json\", \"w\") as file:\n",
    "    file.write(json.dumps(new_results))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.77242408767743"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_bleu_scores = [result[\"bleu_score\"] for result in new_results] \n",
    "np.mean(new_bleu_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.97"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_retrieval_scores = [result[\"retrieval_score\"] for result in new_results] \n",
    "np.mean(new_retrieval_scores)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
