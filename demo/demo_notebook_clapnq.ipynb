{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from copy import deepcopy\n",
    "import json\n",
    "import re\n",
    "\n",
    "from typing import List\n",
    "\n",
    "from datasets import load_dataset, Dataset\n",
    "\n",
    "from nltk import word_tokenize\n",
    "from nltk.translate.bleu_score import sentence_bleu, SmoothingFunction\n",
    "\n",
    "from langchain_community.document_loaders import DataFrameLoader\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from langchain_community.vectorstores.faiss import DistanceStrategy\n",
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.documents.base import Document\n",
    "from langchain_openai import ChatOpenAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "OPENAI_API_KEY = \"\"\n",
    "OPENAI_MODEL = \"gpt-3.5-turbo\"\n",
    "\n",
    "EMBEDDING_MODEL = \"sentence-transformers/all-MiniLM-L6-v2\"\n",
    "\n",
    "DATASET = \"PrimeQA/clapnq\"\n",
    "\n",
    "FAISS_PATH = \"./vectorstore/clapnq\"\n",
    "RESULTS_PATH = \"./results/clapnq\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Embedding Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_model = HuggingFaceEmbeddings(\n",
    "    model_name=EMBEDDING_MODEL, \n",
    "    model_kwargs={\"device\": \"cpu\"}\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['id', 'input', 'passages', 'output'],\n",
       "    num_rows: 201\n",
       "})"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "full_dataset = load_dataset(DATASET, split=\"validation\", name=\"default\")\n",
    "\n",
    "dataset = Dataset.from_list([row for row in full_dataset if row[\"output\"][0][\"answer\"] != \"\"])\n",
    "dataset = dataset.select(range(100, 300))\n",
    "\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Index: 597 entries, 0 to 599\n",
      "Data columns (total 4 columns):\n",
      " #   Column  Non-Null Count  Dtype \n",
      "---  ------  --------------  ----- \n",
      " 0   id      597 non-null    object\n",
      " 1   title   597 non-null    object\n",
      " 2   text    597 non-null    object\n",
      " 3   type    597 non-null    object\n",
      "dtypes: object(4)\n",
      "memory usage: 23.3+ KB\n"
     ]
    }
   ],
   "source": [
    "document_df = pd.DataFrame({\n",
    "    \"id\": full_dataset[\"id\"],\n",
    "    \"title\": [passage[0][\"title\"] for passage in full_dataset[\"passages\"]],\n",
    "    \"text\": [passage[0][\"text\"] for passage in full_dataset[\"passages\"]],\n",
    "    \"type\": \"chunk\"\n",
    "})\n",
    "\n",
    "document_df = document_df.drop_duplicates([\"text\"])\n",
    "document_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "loader = DataFrameLoader(document_df, page_content_column=\"text\")\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size = 300,\n",
    "    chunk_overlap = 100\n",
    ")\n",
    "\n",
    "document_chunks = loader.load()\n",
    "# document_chunks = text_splitter.split_documents(document_chunks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorstore_db = FAISS.from_documents(\n",
    "    documents=document_chunks, \n",
    "    embedding=embedding_model, \n",
    "    normalize_L2=True\n",
    ")\n",
    "\n",
    "vectorstore_db.save_local(FAISS_PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Retriever"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "chunks_vectorstore_db = FAISS.load_local(\n",
    "    folder_path=FAISS_PATH, \n",
    "    embeddings=embedding_model, \n",
    "    normalize_L2=True,\n",
    "    allow_dangerous_deserialization=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "question_vectorstore_db = FAISS.from_documents(\n",
    "    documents=[\n",
    "        Document(page_content=\"Mock Question\", metadata={\n",
    "            \"id\": 0, \n",
    "            \"type\": \"question\", \n",
    "            \"connections\": []\n",
    "        })],\n",
    "    embedding=embedding_model\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LLM Client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm_agent = ChatOpenAI(\n",
    "    api_key=OPENAI_API_KEY,\n",
    "    model=OPENAI_MODEL,\n",
    "    temperature=0.0\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RAG Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RAGAgent:\n",
    "    def __init__(\n",
    "        self,\n",
    "        client: ChatOpenAI,\n",
    "        chunk_retriever: FAISS,\n",
    "        question_retriever: FAISS,\n",
    "        dataframe: pd.DataFrame\n",
    "    ):\n",
    "        self.dataframe = dataframe\n",
    "        self.client = client\n",
    "        self.chunk_retriever = chunk_retriever\n",
    "        self.question_retriever = question_retriever\n",
    "\n",
    "    \n",
    "    def retrieve_context(\n",
    "        self,\n",
    "        query: str,\n",
    "        top_k: int,\n",
    "        distance_threshold: float,\n",
    "        retrieve_questions: bool = False,\n",
    "        **krawgs\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Retrieve relevant documents given query\n",
    "        \"\"\"\n",
    "        retriever = self.chunk_retriever if not retrieve_questions else self.question_retriever\n",
    "        \n",
    "        docs_with_metadata = retriever.similarity_search_with_score(\n",
    "            query=query,\n",
    "            k=top_k,\n",
    "            **krawgs\n",
    "        )\n",
    "        filtered_docs = [doc for doc, score in docs_with_metadata if score <= distance_threshold] \n",
    "        \n",
    "        if retrieve_questions:\n",
    "            doc_ids = [doc for relevant_docs in filtered_docs for doc in relevant_docs.metadata[\"connections\"]]\n",
    "        else:\n",
    "            doc_ids = [doc.metadata[\"id\"] for doc in filtered_docs]\n",
    "\n",
    "        filtered_docs = [DataFrameLoader(pd.DataFrame(self.dataframe[self.dataframe[\"id\"] == doc_id])).load()[0] for doc_id in set(doc_ids)]\n",
    "        \n",
    "        return filtered_docs\n",
    "    \n",
    "\n",
    "    def generate_response(\n",
    "        self,\n",
    "        question: str,\n",
    "        retrieved_docs: List[Document]\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Generate response based on query and context documents\n",
    "        \"\"\"\n",
    "        documents = [\"Title:\" + str(doc.metadata[\"title\"]) + \"\\n\" + str(doc.page_content) for doc in retrieved_docs]\n",
    "        context_str = \"\\n\\n\".join(documents)\n",
    "    \n",
    "        prompt = ChatPromptTemplate.from_messages([\n",
    "            # (\"system\", \"\"\"\n",
    "            #     Answer the users QUESTION using the CONTEXT text above. \n",
    "            #     Keep your answer short, direct, and relevant to the QUESTION. Strictly ground your answers to the sentences in the provided CONTEXT.\n",
    "            #     If the CONTENT doesn't contain the necessary facts to answer the QUESTION, just say \"I don't know\".\n",
    "            #  \"\"\"),\n",
    "             (\"system\", \"\"\"\n",
    "                ### INSTRUCTION\n",
    "                Answer the users QUESTION with the CONTEXT text above. \n",
    "                Strictly ground your answer to the provided CONTEXT. You should use the sentences or keywords from the provided CONTEXT to form your answer.\n",
    "                Keep your answer concise and relevant to the QUESTION.\n",
    "              \"\"\"),\n",
    "            (\"human\", \"### CONTEXT\\n{context}\\n### QUESTION\\n{question}\")\n",
    "        ])\n",
    "\n",
    "        chain = prompt | self.client\n",
    "        response = chain.invoke({\"question\": question, \"context\": context_str})\n",
    "\n",
    "        return response.content\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "rag_agent = RAGAgent(\n",
    "    client=llm_agent,\n",
    "    chunk_retriever=chunks_vectorstore_db,\n",
    "    question_retriever=question_vectorstore_db,\n",
    "    dataframe=document_df\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_bleu_score(ref: str, cand: str):\n",
    "    smoothing_func = SmoothingFunction().method1\n",
    "\n",
    "    reference = word_tokenize(re.sub(r'[^\\w\\s]','', ref.lower()))\n",
    "    candidate = word_tokenize(re.sub(r'[^\\w\\s]','', cand.lower()))\n",
    "\n",
    "    weight_configs = (1, 0, 0, 0)\n",
    "\n",
    "    bleu_score = sentence_bleu(\n",
    "        references=[reference], \n",
    "        hypothesis=candidate, \n",
    "        weights=weight_configs, \n",
    "        smoothing_function=smoothing_func\n",
    "    )\n",
    "\n",
    "    return bleu_score\n",
    "\n",
    "\n",
    "def compute_retrieval_score(ground_truth_context: str, retrieved_context: List[Document]):\n",
    "    retrieved_context_str = [doc.page_content for doc in retrieved_context]\n",
    "    return 1 if ground_truth_context in retrieved_context_str else 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluation(\n",
    "    agent: RAGAgent, \n",
    "    test_set: Dataset, \n",
    "    top_k: int, \n",
    "    q_top_k: int,\n",
    "    distance_threshold: float,\n",
    "    q_distance_threshold: float\n",
    "):\n",
    "    results = []\n",
    "\n",
    "    for i in range(len(test_set)):\n",
    "        id = test_set[i][\"id\"]\n",
    "        question = test_set[i][\"input\"]\n",
    "        ground_truth_answer = test_set[i][\"output\"][0][\"answer\"]\n",
    "        ground_truth_context = test_set[i][\"passages\"][0][\"text\"]\n",
    "\n",
    "        relevant_chunks = agent.retrieve_context(\n",
    "            query=question, \n",
    "            top_k=top_k, \n",
    "            distance_threshold=distance_threshold,\n",
    "            retrieve_questions=False\n",
    "        )\n",
    "\n",
    "        relevant_question_chunks = agent.retrieve_context(\n",
    "            query=question, \n",
    "            top_k=q_top_k, \n",
    "            distance_threshold=q_distance_threshold,\n",
    "            retrieve_questions=True\n",
    "        )\n",
    "\n",
    "        context = relevant_question_chunks + relevant_chunks\n",
    "        response = agent.generate_response(question, context)\n",
    "\n",
    "        bleu_score = compute_bleu_score(ground_truth_answer, response)\n",
    "        retrieval_score = compute_retrieval_score(ground_truth_context, context)\n",
    "\n",
    "        results.append({\n",
    "            \"question\": question,\n",
    "            \"answer\": response,\n",
    "            \"ground_truth_answer\": ground_truth_answer,\n",
    "            \"context\": [chunk.page_content for chunk in relevant_chunks],\n",
    "            \"q_context\": [chunk.page_content for chunk in relevant_question_chunks],\n",
    "            \"ground_truth_context\": ground_truth_context,\n",
    "            \"bleu_score\": bleu_score,\n",
    "            \"retrieval_score\": retrieval_score,\n",
    "            \"id\": id\n",
    "        })\n",
    "        \n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'question': 'what is meant by sanctification in the bible',\n",
       "  'answer': 'Sanctification in the Bible refers to the act or process of acquiring sanctity, of being made or becoming holy. It is a gift given through the power of God to a person or thing which is then considered sacred or set apart in an official capacity within the religion. To sanctify is to literally \"set apart for particular use in a special purpose or work and to make holy or sacred.\"',\n",
       "  'ground_truth_answer': \"Sanctification is the act or process of acquiring sanctity , of being made or becoming holy .\\nTo sanctify is to literally `` set apart for particular use in a special purpose or work and to make holy or sacred . ''\",\n",
       "  'context': [\"Sanctification is the act or process of acquiring sanctity , of being made or becoming holy . It is a gift given through the power of God to a person or thing which is then considered sacred or set apart in an official capacity within the religion . In general anything from a temple , to vessels , to days of the week , to a human believer who willingly accepts this gift can be sanctified . To sanctify is to literally `` set apart for particular use in a special purpose or work and to make holy or sacred . '' Etymologically , `` sanctify '' derives from the Latin verb sanctificare which in turn derives from sanctus `` holy '' and facere `` to make '' . \",\n",
       "   \"`` As a dog returns to his vomit , so a fool repeats his folly '' is an aphorism which appears in the Book of Proverbs in the Bible -- Proverbs 26 : 11 ( Hebrew : כְּ֭כֶלֶב שָׁ֣ב עַל ־ קֵאֹ֑ו כְּ֝סִ֗יל שֹׁונֶ֥ה בְאִוַּלְתֹּֽו \\u200e Kəḵeleḇ šāḇ ' al - qê'ōw ; kəsîl , šōwneh ḇə'iwwaltōw . ) , also partially quoted in the New Testament , 2 Peter 2 : 22 . It means that fools are stubbornly inflexible and this is illustrated with the repulsive simile of the dog that eats its vomit again , even though this may be poisonous . Dogs were considered unclean in Biblical times as they were commonly scavengers of the dead and they appear in the Bible as repugnant creatures , symbolising evil . The reference to vomit indicates excessive indulgence and so also symbolises revulsion . \",\n",
       "   \"In the next narrative dialogue , God questions the man and the woman ( Genesis 3 : 8 -- 13 ) , and God initiates a dialogue by calling out to the man with a rhetorical question designed to consider his wrongdoing . The man explains that he hid in the garden out of fear because he realized his own nakedness ( Genesis 3 : 10 ) . This is followed by two more rhetorical questions designed to show awareness of a defiance of God 's command . The man then points to the woman as the real offender , and he implies that God is responsible for the tragedy because the woman was given to him by God ( Genesis 3 : 12 ) . God challenges the woman to explain herself , whereby she shifts the blame to the serpent ( Genesis 3 : 13 ) . The\",\n",
       "   \"The origin of this custom is unknown , although many reasons have been given . The primary reason is that joy must always be tempered . This is based on two accounts in the Talmud of rabbis who , upon seeing that their son 's wedding celebration was getting out of hand , broke a vessel -- in the second case a glass -- to calm things down . Another explanation is that it is a reminder that despite the joy , Jews still mourn the destruction of the Temple in Jerusalem . Because of this , some recite the verses `` If I forget thee / O Jerusalem ... '' ( Ps. 137 : 5 ) at this point . Many other reasons have been given by traditional authorities . \",\n",
       "   \"Christina Rossetti was a Victorian poet who believed the sensual excitement of the natural world found its meaningful purpose in death and in God . Her The Face of the Deep is a meditation upon the Apocalypse . In her view , what Revelation has to teach is patience . Patience is the closest to perfection the human condition allows . Her book , which is largely written in prose , frequently breaks into poetry or jubilation , much like Revelation itself . The relevance of John 's visions belongs to Christians of all times as a continuous present meditation . Such matters are eternal and outside of normal human reckoning . `` That winter which will be the death of Time has no promise of termination . Winter that returns not to spring ... -- who can bear it ? '' She dealt deftly with the vengeful aspects of John 's message . `` A few are charged to do judgment ; everyone without exception is charged to show mercy . '' Her conclusion is that Christians should see John as `` representative of all his brethren '' so they should `` hope as he hoped , love as he loved . ''\"],\n",
       "  'q_context': [],\n",
       "  'ground_truth_context': \"Sanctification is the act or process of acquiring sanctity , of being made or becoming holy . It is a gift given through the power of God to a person or thing which is then considered sacred or set apart in an official capacity within the religion . In general anything from a temple , to vessels , to days of the week , to a human believer who willingly accepts this gift can be sanctified . To sanctify is to literally `` set apart for particular use in a special purpose or work and to make holy or sacred . '' Etymologically , `` sanctify '' derives from the Latin verb sanctificare which in turn derives from sanctus `` holy '' and facere `` to make '' . \",\n",
       "  'bleu_score': 0.5211267605633803,\n",
       "  'retrieval_score': 1,\n",
       "  'id': '-2652183708580968768'}]"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results = evaluation(\n",
    "    agent=rag_agent,\n",
    "    test_set=dataset,\n",
    "    top_k=5,\n",
    "    q_top_k=1,\n",
    "    distance_threshold=1.5,\n",
    "    q_distance_threshold=0.25\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(f\"{RESULTS_PATH}/run-0.json\", \"w\") as file:\n",
    "    file.write(json.dumps(results))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.4382711866509373"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bleu_scores = [result[\"bleu_score\"] for result in results] \n",
    "np.mean(bleu_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9950248756218906"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "retrieval_scores = [result[\"retrieval_score\"] for result in results] \n",
    "np.mean(retrieval_scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q-RAG Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_question_vectorstore_db = deepcopy(question_vectorstore_db)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, score in enumerate(bleu_scores):\n",
    "    if score < 0.5:\n",
    "        id = dataset[i][\"id\"]\n",
    "        question = dataset[i][\"input\"]\n",
    "        ground_truth_answer = (\" \").join(dataset[i][\"output\"][0][\"answer\"])\n",
    "\n",
    "        relevant_chunk_list = rag_agent.retrieve_context(\n",
    "            query=question + \" \" + ground_truth_answer,\n",
    "            top_k=2,\n",
    "            distance_threshold=1.5\n",
    "        )\n",
    "\n",
    "        relevant_chunk_id_list = [chunk.metadata[\"id\"] for chunk in relevant_chunk_list]\n",
    "\n",
    "        question_document = Document(\n",
    "            page_content=question,\n",
    "            metadata={\n",
    "                \"id\": id,\n",
    "                \"type\": \"question\", \n",
    "                \"connections\": relevant_chunk_id_list\n",
    "            }\n",
    "        )\n",
    "\n",
    "        new_question_vectorstore_db.add_documents([question_document])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_rag_agent = RAGAgent(\n",
    "    client=llm_agent,\n",
    "    chunk_retriever=chunks_vectorstore_db,\n",
    "    question_retriever=new_question_vectorstore_db,\n",
    "    dataframe=document_df\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_results = evaluation(\n",
    "    agent=new_rag_agent,\n",
    "    test_set=dataset,\n",
    "    top_k=5,\n",
    "    q_top_k=2,\n",
    "    distance_threshold=1.5,\n",
    "    q_distance_threshold=0.25\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(f\"{RESULTS_PATH}/run-1.json\", \"w\") as file:\n",
    "    file.write(json.dumps(new_results))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.4449976317072134"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_bleu_scores = [result[\"bleu_score\"] for result in new_results] \n",
    "np.mean(new_bleu_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9950248756218906"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_retrieval_scores = [result[\"retrieval_score\"] for result in new_results] \n",
    "np.mean(new_retrieval_scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analysis"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
